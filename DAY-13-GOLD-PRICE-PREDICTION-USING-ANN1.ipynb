{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "598a5842-5322-41b0-8196-a1346365c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd9fa11-887c-4d85-ad02-d9ad534bcc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "591e2a04-0116-45da-bd68-2c8ea694b914",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gld_price_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff56501-b368-4218-8fa5-99ef0c4669b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SPX</th>\n",
       "      <th>GLD</th>\n",
       "      <th>USO</th>\n",
       "      <th>SLV</th>\n",
       "      <th>EUR/USD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/02/08</td>\n",
       "      <td>1447.160034</td>\n",
       "      <td>84.860001</td>\n",
       "      <td>78.470001</td>\n",
       "      <td>15.180</td>\n",
       "      <td>1.471692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/03/08</td>\n",
       "      <td>1447.160034</td>\n",
       "      <td>85.570000</td>\n",
       "      <td>78.370003</td>\n",
       "      <td>15.285</td>\n",
       "      <td>1.474491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/04/08</td>\n",
       "      <td>1411.630005</td>\n",
       "      <td>85.129997</td>\n",
       "      <td>77.309998</td>\n",
       "      <td>15.167</td>\n",
       "      <td>1.475492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/07/08</td>\n",
       "      <td>1416.180054</td>\n",
       "      <td>84.769997</td>\n",
       "      <td>75.500000</td>\n",
       "      <td>15.053</td>\n",
       "      <td>1.468299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/08/08</td>\n",
       "      <td>1390.189941</td>\n",
       "      <td>86.779999</td>\n",
       "      <td>76.059998</td>\n",
       "      <td>15.590</td>\n",
       "      <td>1.557099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date          SPX        GLD        USO     SLV   EUR/USD\n",
       "0  01/02/08  1447.160034  84.860001  78.470001  15.180  1.471692\n",
       "1  01/03/08  1447.160034  85.570000  78.370003  15.285  1.474491\n",
       "2  01/04/08  1411.630005  85.129997  77.309998  15.167  1.475492\n",
       "3  01/07/08  1416.180054  84.769997  75.500000  15.053  1.468299\n",
       "4  01/08/08  1390.189941  86.779999  76.059998  15.590  1.557099"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2147b122-0b34-4ec0-b177-91f119065221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining dependent and the independent variables\n",
    "x = df[['SPX','USO','SLV','EUR/USD']]\n",
    "y = df['GLD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a47352d-04c2-471f-92a0-c214fd040ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c53f2af-0857-48bf-9e0d-fa82ab59ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "638422e1-3cae-405b-aba5-93fe54b6dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0312ca15-edc0-4e57-a133-eb9e717581da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa24fd21-781b-45ae-9212-0bd5c884782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(10,activation='relu',input_dim=4))\n",
    "model.add(Dense(10,activation='relu'))\n",
    "model.add(Dense(1,activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f750a2d3-af02-4eab-b858-473dd9e67f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\",optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ede7f1f-ed58-428e-9d97-ca326ad83f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 90.7076 - val_loss: 104.5896\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 88.4881 - val_loss: 102.1421\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 86.8718 - val_loss: 99.8300\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 85.0197 - val_loss: 98.8176\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 83.5262 - val_loss: 97.1415\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 82.1295 - val_loss: 96.2135\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 80.8218 - val_loss: 93.4128\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 79.4760 - val_loss: 92.2934\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 78.6079 - val_loss: 90.7380\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 77.5094 - val_loss: 89.4073\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 76.6939 - val_loss: 88.6005\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 76.1129 - val_loss: 87.6215\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 75.3004 - val_loss: 87.0551\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 74.7004 - val_loss: 86.5349\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 74.0344 - val_loss: 86.2850\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 73.5885 - val_loss: 84.5777\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 73.0212 - val_loss: 84.4103\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 72.5557 - val_loss: 83.6902\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 72.1322 - val_loss: 82.9588\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 71.8725 - val_loss: 82.3710\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 71.5031 - val_loss: 81.8529\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 71.1526 - val_loss: 81.7000\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 70.6837 - val_loss: 81.1759\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 70.3466 - val_loss: 80.8373\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 70.0623 - val_loss: 80.7628\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 69.7775 - val_loss: 79.9044\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 69.5620 - val_loss: 79.8691\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 69.3400 - val_loss: 80.3558\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 69.1642 - val_loss: 78.8665\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 68.9820 - val_loss: 79.2042\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 68.7104 - val_loss: 79.0638\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 68.4840 - val_loss: 77.8869\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 68.2291 - val_loss: 78.4171\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 68.2219 - val_loss: 77.8681\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 67.9092 - val_loss: 77.5107\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 67.7387 - val_loss: 77.1157\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 67.7088 - val_loss: 77.0151\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 67.7099 - val_loss: 77.0226\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 67.3518 - val_loss: 76.3738\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 67.1251 - val_loss: 76.3168\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 67.1556 - val_loss: 76.6786\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 67.1518 - val_loss: 75.9060\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 66.8777 - val_loss: 76.2291\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 66.8164 - val_loss: 75.8189\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 66.8415 - val_loss: 76.1846\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 66.6911 - val_loss: 75.8727\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 66.5296 - val_loss: 75.5327\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 66.6665 - val_loss: 74.9543\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 66.3966 - val_loss: 74.7205\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 66.0242 - val_loss: 74.9152\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 66.3183 - val_loss: 74.9235\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 66.0989 - val_loss: 74.5776\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 66.0652 - val_loss: 74.4679\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 65.6872 - val_loss: 74.1275\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 65.9489 - val_loss: 73.8231\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 65.7957 - val_loss: 73.9907\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 65.6623 - val_loss: 74.2173\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 65.5945 - val_loss: 75.7102\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 65.7564 - val_loss: 73.8553\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 65.4231 - val_loss: 73.3687\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 65.3545 - val_loss: 73.6715\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 65.2889 - val_loss: 73.8445\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 65.3776 - val_loss: 73.1604\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 65.2492 - val_loss: 73.4922\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 64.9637 - val_loss: 73.2020\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 64.9375 - val_loss: 72.8821\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 64.9980 - val_loss: 73.2822\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 64.9229 - val_loss: 75.1630\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 64.7709 - val_loss: 72.9849\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 64.9279 - val_loss: 72.6052\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 64.6850 - val_loss: 72.6905\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 64.7449 - val_loss: 72.4047\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 64.7070 - val_loss: 72.3098\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 64.7388 - val_loss: 73.1282\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 64.5713 - val_loss: 73.1872\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 64.4315 - val_loss: 72.6213\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 64.6408 - val_loss: 71.9276\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 64.4098 - val_loss: 72.1634\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 64.5381 - val_loss: 71.8797\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 64.1984 - val_loss: 72.1935\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 63.9824 - val_loss: 71.7282\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 63.8605 - val_loss: 71.3419\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 63.8371 - val_loss: 71.5268\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 64.0086 - val_loss: 71.7524\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 63.6553 - val_loss: 71.6479\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 63.9035 - val_loss: 71.3522\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 63.5388 - val_loss: 71.3917\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 63.4316 - val_loss: 71.0632\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 63.2576 - val_loss: 70.7733\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 63.2178 - val_loss: 70.7408\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 63.1228 - val_loss: 70.7854\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 63.2444 - val_loss: 70.2980\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 62.9419 - val_loss: 71.4174\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 62.9934 - val_loss: 70.4643\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 62.9580 - val_loss: 70.3956\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 62.9972 - val_loss: 70.5416\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 62.9683 - val_loss: 70.2071\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 62.8383 - val_loss: 69.8241\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 62.5869 - val_loss: 69.9509\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 62.3608 - val_loss: 70.4278\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train_scaled,y_train,epochs=100,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7df4d5b-bcd8-4bb7-90f7-42172f98d172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1f5b780-2c36-45c6-9f74-31d8b7a8d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c4e41a8-0f75-409a-9aad-b8ef2112b6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9036308311218227"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916c3bba-025c-49f8-a3cb-d725d4afdef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
